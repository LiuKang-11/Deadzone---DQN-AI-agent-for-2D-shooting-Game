{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d14e7c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/liuzitang/Desktop/CSC584/Project/Deadzone/src/EnvironmentSimulator.py\n"
     ]
    }
   ],
   "source": [
    "import EnvironmentSimulator\n",
    "print(EnvironmentSimulator.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a879f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "970b5463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Episode 0 Summary ===\n",
      "Player  Reward: 4414.90\n",
      "  Hits: 11, Misses: 347, Kills: 2\n",
      "  Moves: 818, TurnL: 94, TurnR: 31, Switch: 80\n",
      "Opponent Reward: 200.80\n",
      "  Hits: 1, Misses: 123, Kills: 0\n",
      "  Moves: 964, TurnL: 78, TurnR: 117, Switch: 98\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 1 Summary ===\n",
      "Player  Reward: -483.30\n",
      "  Hits: 0, Misses: 880, Kills: 0\n",
      "  Moves: 1132, TurnL: 257, TurnR: 148, Switch: 83\n",
      "Opponent Reward: -517.90\n",
      "  Hits: 0, Misses: 4, Kills: 0\n",
      "  Moves: 2375, TurnL: 12, TurnR: 74, Switch: 35\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 2 Summary ===\n",
      "Player  Reward: 4063.10\n",
      "  Hits: 11, Misses: 501, Kills: 2\n",
      "  Moves: 1047, TurnL: 68, TurnR: 44, Switch: 93\n",
      "Opponent Reward: -443.20\n",
      "  Hits: 0, Misses: 3, Kills: 0\n",
      "  Moves: 1753, TurnL: 6, TurnR: 2, Switch: 0\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 3 Summary ===\n",
      "Player  Reward: 4283.20\n",
      "  Hits: 11, Misses: 1312, Kills: 2\n",
      "  Moves: 537, TurnL: 26, TurnR: 85, Switch: 79\n",
      "Opponent Reward: -485.60\n",
      "  Hits: 0, Misses: 34, Kills: 0\n",
      "  Moves: 2012, TurnL: 1, TurnR: 2, Switch: 1\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 4 Summary ===\n",
      "Player  Reward: 4201.80\n",
      "  Hits: 11, Misses: 202, Kills: 2\n",
      "  Moves: 28, TurnL: 0, TurnR: 0, Switch: 0\n",
      "Opponent Reward: -128.60\n",
      "  Hits: 0, Misses: 109, Kills: 0\n",
      "  Moves: 132, TurnL: 0, TurnR: 0, Switch: 0\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 5 Summary ===\n",
      "Player  Reward: -448.70\n",
      "  Hits: 0, Misses: 3, Kills: 0\n",
      "  Moves: 2444, TurnL: 0, TurnR: 40, Switch: 13\n",
      "Opponent Reward: -502.90\n",
      "  Hits: 0, Misses: 2, Kills: 0\n",
      "  Moves: 2493, TurnL: 1, TurnR: 2, Switch: 2\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 6 Summary ===\n",
      "Player  Reward: -573.10\n",
      "  Hits: 0, Misses: 2, Kills: 0\n",
      "  Moves: 2413, TurnL: 39, TurnR: 19, Switch: 27\n",
      "Opponent Reward: -626.40\n",
      "  Hits: 0, Misses: 53, Kills: 0\n",
      "  Moves: 1901, TurnL: 1, TurnR: 544, Switch: 1\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 7 Summary ===\n",
      "Player  Reward: -748.40\n",
      "  Hits: 0, Misses: 831, Kills: 0\n",
      "  Moves: 1652, TurnL: 14, TurnR: 2, Switch: 1\n",
      "Opponent Reward: -912.30\n",
      "  Hits: 0, Misses: 47, Kills: 0\n",
      "  Moves: 2387, TurnL: 45, TurnR: 1, Switch: 20\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 8 Summary ===\n",
      "Player  Reward: -622.10\n",
      "  Hits: 0, Misses: 940, Kills: 0\n",
      "  Moves: 1408, TurnL: 148, TurnR: 3, Switch: 1\n",
      "Opponent Reward: -195.10\n",
      "  Hits: 0, Misses: 1, Kills: 0\n",
      "  Moves: 2328, TurnL: 1, TurnR: 1, Switch: 169\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 9 Summary ===\n",
      "Player  Reward: 87.30\n",
      "  Hits: 0, Misses: 3, Kills: 0\n",
      "  Moves: 613, TurnL: 280, TurnR: 336, Switch: 1268\n",
      "Opponent Reward: -632.20\n",
      "  Hits: 1, Misses: 3, Kills: 0\n",
      "  Moves: 2490, TurnL: 3, TurnR: 2, Switch: 1\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 10 Summary ===\n",
      "Player  Reward: -71.60\n",
      "  Hits: 0, Misses: 708, Kills: 0\n",
      "  Moves: 1086, TurnL: 3, TurnR: 200, Switch: 503\n",
      "Opponent Reward: -163.40\n",
      "  Hits: 0, Misses: 3, Kills: 0\n",
      "  Moves: 2478, TurnL: 0, TurnR: 6, Switch: 13\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 11 Summary ===\n",
      "Player  Reward: 405307.00\n",
      "  Hits: 67, Misses: 528, Kills: 64\n",
      "  Moves: 1379, TurnL: 22, TurnR: 221, Switch: 283\n",
      "Opponent Reward: -90163.50\n",
      "  Hits: 0, Misses: 0, Kills: 0\n",
      "  Moves: 2259, TurnL: 81, TurnR: 103, Switch: 57\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 12 Summary ===\n",
      "Player  Reward: -98.70\n",
      "  Hits: 0, Misses: 2, Kills: 0\n",
      "  Moves: 460, TurnL: 56, TurnR: 1233, Switch: 749\n",
      "Opponent Reward: -573.00\n",
      "  Hits: 0, Misses: 3, Kills: 0\n",
      "  Moves: 2383, TurnL: 2, TurnR: 110, Switch: 2\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 13 Summary ===\n",
      "Player  Reward: -95.10\n",
      "  Hits: 0, Misses: 390, Kills: 0\n",
      "  Moves: 898, TurnL: 3, TurnR: 1201, Switch: 8\n",
      "Opponent Reward: -457.10\n",
      "  Hits: 0, Misses: 2, Kills: 0\n",
      "  Moves: 2493, TurnL: 0, TurnR: 2, Switch: 3\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 14 Summary ===\n",
      "Player  Reward: -372.90\n",
      "  Hits: 0, Misses: 10, Kills: 0\n",
      "  Moves: 2216, TurnL: 2, TurnR: 269, Switch: 3\n",
      "Opponent Reward: -433.60\n",
      "  Hits: 0, Misses: 85, Kills: 0\n",
      "  Moves: 2388, TurnL: 2, TurnR: 25, Switch: 0\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 15 Summary ===\n",
      "Player  Reward: 27.00\n",
      "  Hits: 0, Misses: 2389, Kills: 0\n",
      "  Moves: 65, TurnL: 3, TurnR: 0, Switch: 43\n",
      "Opponent Reward: -409.70\n",
      "  Hits: 0, Misses: 0, Kills: 0\n",
      "  Moves: 2395, TurnL: 2, TurnR: 20, Switch: 83\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 16 Summary ===\n",
      "Player  Reward: -847.40\n",
      "  Hits: 0, Misses: 2, Kills: 0\n",
      "  Moves: 2061, TurnL: 2, TurnR: 3, Switch: 432\n",
      "Opponent Reward: -855.00\n",
      "  Hits: 0, Misses: 99, Kills: 0\n",
      "  Moves: 2310, TurnL: 10, TurnR: 25, Switch: 56\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 17 Summary ===\n",
      "Player  Reward: -389.50\n",
      "  Hits: 0, Misses: 351, Kills: 0\n",
      "  Moves: 2012, TurnL: 2, TurnR: 132, Switch: 3\n",
      "Opponent Reward: -509.30\n",
      "  Hits: 0, Misses: 4, Kills: 0\n",
      "  Moves: 2489, TurnL: 2, TurnR: 3, Switch: 2\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 18 Summary ===\n",
      "Player  Reward: 4125.80\n",
      "  Hits: 11, Misses: 178, Kills: 2\n",
      "  Moves: 476, TurnL: 0, TurnR: 28, Switch: 1\n",
      "Opponent Reward: -201.00\n",
      "  Hits: 0, Misses: 1, Kills: 0\n",
      "  Moves: 675, TurnL: 1, TurnR: 17, Switch: 0\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 19 Summary ===\n",
      "Player  Reward: -166.00\n",
      "  Hits: 0, Misses: 1050, Kills: 0\n",
      "  Moves: 1373, TurnL: 2, TurnR: 73, Switch: 2\n",
      "Opponent Reward: -468.80\n",
      "  Hits: 0, Misses: 1, Kills: 0\n",
      "  Moves: 2487, TurnL: 2, TurnR: 8, Switch: 2\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 20 Summary ===\n",
      "Player  Reward: 203.80\n",
      "  Hits: 0, Misses: 2158, Kills: 0\n",
      "  Moves: 185, TurnL: 153, TurnR: 2, Switch: 2\n",
      "Opponent Reward: -400.40\n",
      "  Hits: 0, Misses: 33, Kills: 0\n",
      "  Moves: 2440, TurnL: 1, TurnR: 2, Switch: 24\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 21 Summary ===\n",
      "Player  Reward: -221.00\n",
      "  Hits: 0, Misses: 337, Kills: 0\n",
      "  Moves: 1200, TurnL: 484, TurnR: 296, Switch: 183\n",
      "Opponent Reward: -401.30\n",
      "  Hits: 0, Misses: 3, Kills: 0\n",
      "  Moves: 2408, TurnL: 74, TurnR: 2, Switch: 13\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 22 Summary ===\n",
      "Player  Reward: -134.00\n",
      "  Hits: 0, Misses: 194, Kills: 0\n",
      "  Moves: 702, TurnL: 1230, TurnR: 241, Switch: 133\n",
      "Opponent Reward: -302.50\n",
      "  Hits: 0, Misses: 2, Kills: 0\n",
      "  Moves: 684, TurnL: 283, TurnR: 2, Switch: 1529\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 23 Summary ===\n",
      "Player  Reward: -525.20\n",
      "  Hits: 0, Misses: 818, Kills: 0\n",
      "  Moves: 933, TurnL: 380, TurnR: 239, Switch: 130\n",
      "Opponent Reward: -866.90\n",
      "  Hits: 0, Misses: 1, Kills: 0\n",
      "  Moves: 2471, TurnL: 1, TurnR: 21, Switch: 6\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 24 Summary ===\n",
      "Player  Reward: 27.20\n",
      "  Hits: 0, Misses: 2, Kills: 0\n",
      "  Moves: 1120, TurnL: 426, TurnR: 104, Switch: 848\n",
      "Opponent Reward: -411.60\n",
      "  Hits: 0, Misses: 85, Kills: 0\n",
      "  Moves: 2359, TurnL: 52, TurnR: 3, Switch: 1\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 25 Summary ===\n",
      "Player  Reward: -8.60\n",
      "  Hits: 0, Misses: 2, Kills: 0\n",
      "  Moves: 425, TurnL: 181, TurnR: 4, Switch: 1888\n",
      "Opponent Reward: -615.10\n",
      "  Hits: 0, Misses: 214, Kills: 0\n",
      "  Moves: 2277, TurnL: 4, TurnR: 3, Switch: 2\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 26 Summary ===\n",
      "Player  Reward: -953.10\n",
      "  Hits: 0, Misses: 3, Kills: 0\n",
      "  Moves: 2472, TurnL: 17, TurnR: 5, Switch: 3\n",
      "Opponent Reward: -811.50\n",
      "  Hits: 0, Misses: 2, Kills: 0\n",
      "  Moves: 2490, TurnL: 3, TurnR: 2, Switch: 3\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 27 Summary ===\n",
      "Player  Reward: -329.10\n",
      "  Hits: 0, Misses: 166, Kills: 0\n",
      "  Moves: 1288, TurnL: 641, TurnR: 2, Switch: 403\n",
      "Opponent Reward: -503.20\n",
      "  Hits: 0, Misses: 3, Kills: 0\n",
      "  Moves: 2261, TurnL: 2, TurnR: 3, Switch: 231\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 28 Summary ===\n",
      "Player  Reward: -318.90\n",
      "  Hits: 0, Misses: 1, Kills: 0\n",
      "  Moves: 1946, TurnL: 2, TurnR: 206, Switch: 345\n",
      "Opponent Reward: -457.80\n",
      "  Hits: 0, Misses: 0, Kills: 0\n",
      "  Moves: 2492, TurnL: 5, TurnR: 1, Switch: 2\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 29 Summary ===\n",
      "Player  Reward: -170.00\n",
      "  Hits: 0, Misses: 1, Kills: 0\n",
      "  Moves: 1854, TurnL: 220, TurnR: 34, Switch: 391\n",
      "Opponent Reward: -423.10\n",
      "  Hits: 0, Misses: 1, Kills: 0\n",
      "  Moves: 2496, TurnL: 1, TurnR: 1, Switch: 1\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 30 Summary ===\n",
      "Player  Reward: -504.50\n",
      "  Hits: 0, Misses: 1089, Kills: 0\n",
      "  Moves: 708, TurnL: 118, TurnR: 2, Switch: 583\n",
      "Opponent Reward: -772.90\n",
      "  Hits: 4, Misses: 381, Kills: 0\n",
      "  Moves: 1857, TurnL: 51, TurnR: 132, Switch: 75\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 31 Summary ===\n",
      "Player  Reward: -969.40\n",
      "  Hits: 0, Misses: 0, Kills: 0\n",
      "  Moves: 2493, TurnL: 4, TurnR: 1, Switch: 2\n",
      "Opponent Reward: -922.40\n",
      "  Hits: 0, Misses: 63, Kills: 0\n",
      "  Moves: 2432, TurnL: 2, TurnR: 1, Switch: 2\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 32 Summary ===\n",
      "Player  Reward: -491.10\n",
      "  Hits: 0, Misses: 167, Kills: 0\n",
      "  Moves: 2046, TurnL: 12, TurnR: 267, Switch: 8\n",
      "Opponent Reward: -609.30\n",
      "  Hits: 0, Misses: 5, Kills: 0\n",
      "  Moves: 2486, TurnL: 3, TurnR: 5, Switch: 1\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 33 Summary ===\n",
      "Player  Reward: -666.20\n",
      "  Hits: 0, Misses: 603, Kills: 0\n",
      "  Moves: 1540, TurnL: 308, TurnR: 44, Switch: 5\n",
      "Opponent Reward: -461.10\n",
      "  Hits: 0, Misses: 19, Kills: 0\n",
      "  Moves: 2478, TurnL: 1, TurnR: 1, Switch: 1\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 34 Summary ===\n",
      "Player  Reward: -221.00\n",
      "  Hits: 0, Misses: 18, Kills: 0\n",
      "  Moves: 1775, TurnL: 211, TurnR: 65, Switch: 431\n",
      "Opponent Reward: -364.90\n",
      "  Hits: 0, Misses: 3, Kills: 0\n",
      "  Moves: 2433, TurnL: 3, TurnR: 1, Switch: 60\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 35 Summary ===\n",
      "Player  Reward: -298.10\n",
      "  Hits: 0, Misses: 201, Kills: 0\n",
      "  Moves: 1723, TurnL: 162, TurnR: 20, Switch: 394\n",
      "Opponent Reward: -576.30\n",
      "  Hits: 0, Misses: 79, Kills: 0\n",
      "  Moves: 2344, TurnL: 36, TurnR: 20, Switch: 21\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 36 Summary ===\n",
      "Player  Reward: -107.80\n",
      "  Hits: 0, Misses: 534, Kills: 0\n",
      "  Moves: 1545, TurnL: 262, TurnR: 157, Switch: 2\n",
      "Opponent Reward: -398.50\n",
      "  Hits: 0, Misses: 2, Kills: 0\n",
      "  Moves: 2488, TurnL: 2, TurnR: 6, Switch: 2\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 37 Summary ===\n",
      "Player  Reward: -391.30\n",
      "  Hits: 0, Misses: 445, Kills: 0\n",
      "  Moves: 1940, TurnL: 103, TurnR: 11, Switch: 1\n",
      "Opponent Reward: -474.00\n",
      "  Hits: 0, Misses: 2, Kills: 0\n",
      "  Moves: 2492, TurnL: 1, TurnR: 0, Switch: 5\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 38 Summary ===\n",
      "Player  Reward: -952.10\n",
      "  Hits: 0, Misses: 4, Kills: 0\n",
      "  Moves: 2491, TurnL: 1, TurnR: 0, Switch: 4\n",
      "Opponent Reward: -918.90\n",
      "  Hits: 0, Misses: 2, Kills: 0\n",
      "  Moves: 2446, TurnL: 4, TurnR: 0, Switch: 48\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 39 Summary ===\n",
      "Player  Reward: -58.10\n",
      "  Hits: 0, Misses: 17, Kills: 0\n",
      "  Moves: 1727, TurnL: 195, TurnR: 382, Switch: 179\n",
      "Opponent Reward: -334.90\n",
      "  Hits: 0, Misses: 47, Kills: 0\n",
      "  Moves: 2417, TurnL: 13, TurnR: 23, Switch: 0\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 40 Summary ===\n",
      "Player  Reward: -111.90\n",
      "  Hits: 0, Misses: 4, Kills: 0\n",
      "  Moves: 2003, TurnL: 150, TurnR: 288, Switch: 55\n",
      "Opponent Reward: -255.20\n",
      "  Hits: 0, Misses: 75, Kills: 0\n",
      "  Moves: 2200, TurnL: 92, TurnR: 94, Switch: 39\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 41 Summary ===\n",
      "Player  Reward: -198.80\n",
      "  Hits: 0, Misses: 5, Kills: 0\n",
      "  Moves: 1815, TurnL: 205, TurnR: 11, Switch: 464\n",
      "Opponent Reward: -476.80\n",
      "  Hits: 0, Misses: 2, Kills: 0\n",
      "  Moves: 2482, TurnL: 12, TurnR: 3, Switch: 1\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 42 Summary ===\n",
      "Player  Reward: 159.80\n",
      "  Hits: 0, Misses: 351, Kills: 0\n",
      "  Moves: 1276, TurnL: 592, TurnR: 256, Switch: 25\n",
      "Opponent Reward: 53.50\n",
      "  Hits: 0, Misses: 101, Kills: 0\n",
      "  Moves: 1838, TurnL: 127, TurnR: 230, Switch: 204\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 43 Summary ===\n",
      "Player  Reward: 4109.70\n",
      "  Hits: 11, Misses: 112, Kills: 2\n",
      "  Moves: 1111, TurnL: 46, TurnR: 31, Switch: 31\n",
      "Opponent Reward: -131.10\n",
      "  Hits: 0, Misses: 3, Kills: 0\n",
      "  Moves: 567, TurnL: 2, TurnR: 249, Switch: 521\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 44 Summary ===\n",
      "Player  Reward: 262473.10\n",
      "  Hits: 209, Misses: 1447, Kills: 206\n",
      "  Moves: 838, TurnL: 1, TurnR: 2, Switch: 3\n",
      "Opponent Reward: -22740.50\n",
      "  Hits: 25, Misses: 14, Kills: 25\n",
      "  Moves: 2415, TurnL: 8, TurnR: 0, Switch: 38\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 45 Summary ===\n",
      "Player  Reward: -412.30\n",
      "  Hits: 0, Misses: 129, Kills: 0\n",
      "  Moves: 2363, TurnL: 3, TurnR: 5, Switch: 0\n",
      "Opponent Reward: -419.20\n",
      "  Hits: 0, Misses: 16, Kills: 0\n",
      "  Moves: 2479, TurnL: 3, TurnR: 1, Switch: 1\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 46 Summary ===\n",
      "Player  Reward: -632.10\n",
      "  Hits: 0, Misses: 914, Kills: 0\n",
      "  Moves: 1221, TurnL: 0, TurnR: 23, Switch: 342\n",
      "Opponent Reward: -916.60\n",
      "  Hits: 0, Misses: 1, Kills: 0\n",
      "  Moves: 2491, TurnL: 2, TurnR: 3, Switch: 3\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 47 Summary ===\n",
      "Player  Reward: -427.30\n",
      "  Hits: 0, Misses: 569, Kills: 0\n",
      "  Moves: 1890, TurnL: 4, TurnR: 34, Switch: 3\n",
      "Opponent Reward: -746.00\n",
      "  Hits: 0, Misses: 27, Kills: 0\n",
      "  Moves: 2438, TurnL: 5, TurnR: 28, Switch: 2\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 48 Summary ===\n",
      "Player  Reward: 1.70\n",
      "  Hits: 0, Misses: 2087, Kills: 0\n",
      "  Moves: 403, TurnL: 3, TurnR: 5, Switch: 2\n",
      "Opponent Reward: -377.40\n",
      "  Hits: 0, Misses: 46, Kills: 0\n",
      "  Moves: 2420, TurnL: 8, TurnR: 9, Switch: 17\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 49 Summary ===\n",
      "Player  Reward: -139.20\n",
      "  Hits: 0, Misses: 2115, Kills: 0\n",
      "  Moves: 361, TurnL: 22, TurnR: 2, Switch: 0\n",
      "Opponent Reward: -495.70\n",
      "  Hits: 0, Misses: 131, Kills: 0\n",
      "  Moves: 2308, TurnL: 35, TurnR: 23, Switch: 3\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 50 Summary ===\n",
      "Player  Reward: -162.00\n",
      "  Hits: 0, Misses: 1593, Kills: 0\n",
      "  Moves: 793, TurnL: 19, TurnR: 89, Switch: 6\n",
      "Opponent Reward: -594.10\n",
      "  Hits: 5, Misses: 2, Kills: 0\n",
      "  Moves: 2292, TurnL: 6, TurnR: 68, Switch: 127\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 51 Summary ===\n",
      "Player  Reward: 4170.80\n",
      "  Hits: 11, Misses: 83, Kills: 2\n",
      "  Moves: 198, TurnL: 0, TurnR: 23, Switch: 282\n",
      "Opponent Reward: -182.90\n",
      "  Hits: 0, Misses: 70, Kills: 0\n",
      "  Moves: 521, TurnL: 1, TurnR: 0, Switch: 5\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 52 Summary ===\n",
      "Player  Reward: -358.30\n",
      "  Hits: 0, Misses: 1349, Kills: 0\n",
      "  Moves: 417, TurnL: 2, TurnR: 59, Switch: 673\n",
      "Opponent Reward: -870.70\n",
      "  Hits: 0, Misses: 25, Kills: 0\n",
      "  Moves: 2401, TurnL: 65, TurnR: 3, Switch: 6\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 53 Summary ===\n",
      "Player  Reward: -405.20\n",
      "  Hits: 0, Misses: 882, Kills: 0\n",
      "  Moves: 626, TurnL: 4, TurnR: 521, Switch: 467\n",
      "Opponent Reward: -691.80\n",
      "  Hits: 0, Misses: 70, Kills: 0\n",
      "  Moves: 2074, TurnL: 131, TurnR: 165, Switch: 60\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 54 Summary ===\n",
      "Player  Reward: -241.00\n",
      "  Hits: 0, Misses: 5, Kills: 0\n",
      "  Moves: 1072, TurnL: 954, TurnR: 200, Switch: 269\n",
      "Opponent Reward: -616.90\n",
      "  Hits: 0, Misses: 21, Kills: 0\n",
      "  Moves: 2474, TurnL: 1, TurnR: 2, Switch: 2\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 55 Summary ===\n",
      "Player  Reward: -72.80\n",
      "  Hits: 0, Misses: 1812, Kills: 0\n",
      "  Moves: 661, TurnL: 0, TurnR: 0, Switch: 27\n",
      "Opponent Reward: -331.60\n",
      "  Hits: 0, Misses: 14, Kills: 0\n",
      "  Moves: 2373, TurnL: 2, TurnR: 3, Switch: 108\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 56 Summary ===\n",
      "Player  Reward: 48.90\n",
      "  Hits: 0, Misses: 1947, Kills: 0\n",
      "  Moves: 535, TurnL: 10, TurnR: 4, Switch: 4\n",
      "Opponent Reward: -480.40\n",
      "  Hits: 0, Misses: 0, Kills: 0\n",
      "  Moves: 2495, TurnL: 1, TurnR: 1, Switch: 3\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 57 Summary ===\n",
      "Player  Reward: -328.30\n",
      "  Hits: 0, Misses: 165, Kills: 0\n",
      "  Moves: 423, TurnL: 1883, TurnR: 1, Switch: 28\n",
      "Opponent Reward: -760.80\n",
      "  Hits: 0, Misses: 5, Kills: 0\n",
      "  Moves: 2463, TurnL: 29, TurnR: 1, Switch: 2\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 58 Summary ===\n",
      "Player  Reward: -227.10\n",
      "  Hits: 0, Misses: 1787, Kills: 0\n",
      "  Moves: 689, TurnL: 19, TurnR: 4, Switch: 1\n",
      "Opponent Reward: -548.90\n",
      "  Hits: 0, Misses: 2, Kills: 0\n",
      "  Moves: 2487, TurnL: 8, TurnR: 1, Switch: 2\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 59 Summary ===\n",
      "Player  Reward: -129.90\n",
      "  Hits: 0, Misses: 2, Kills: 0\n",
      "  Moves: 2015, TurnL: 352, TurnR: 2, Switch: 129\n",
      "Opponent Reward: 187.90\n",
      "  Hits: 0, Misses: 185, Kills: 0\n",
      "  Moves: 2234, TurnL: 76, TurnR: 4, Switch: 1\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 60 Summary ===\n",
      "Player  Reward: -218.10\n",
      "  Hits: 0, Misses: 4, Kills: 0\n",
      "  Moves: 1816, TurnL: 431, TurnR: 182, Switch: 67\n",
      "Opponent Reward: -545.00\n",
      "  Hits: 0, Misses: 2, Kills: 0\n",
      "  Moves: 2492, TurnL: 3, TurnR: 1, Switch: 2\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 61 Summary ===\n",
      "Player  Reward: -542.50\n",
      "  Hits: 0, Misses: 4, Kills: 0\n",
      "  Moves: 1867, TurnL: 151, TurnR: 105, Switch: 373\n",
      "Opponent Reward: -767.90\n",
      "  Hits: 0, Misses: 1, Kills: 0\n",
      "  Moves: 2425, TurnL: 0, TurnR: 74, Switch: 0\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 62 Summary ===\n",
      "Player  Reward: -274.20\n",
      "  Hits: 0, Misses: 88, Kills: 0\n",
      "  Moves: 1927, TurnL: 114, TurnR: 367, Switch: 4\n",
      "Opponent Reward: -378.40\n",
      "  Hits: 0, Misses: 53, Kills: 0\n",
      "  Moves: 2417, TurnL: 5, TurnR: 22, Switch: 3\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 63 Summary ===\n",
      "Player  Reward: -857.50\n",
      "  Hits: 0, Misses: 34, Kills: 0\n",
      "  Moves: 2344, TurnL: 17, TurnR: 103, Switch: 2\n",
      "Opponent Reward: -863.70\n",
      "  Hits: 0, Misses: 29, Kills: 0\n",
      "  Moves: 2462, TurnL: 1, TurnR: 4, Switch: 4\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 64 Summary ===\n",
      "Player  Reward: -183.50\n",
      "  Hits: 0, Misses: 794, Kills: 0\n",
      "  Moves: 1696, TurnL: 2, TurnR: 2, Switch: 6\n",
      "Opponent Reward: -369.00\n",
      "  Hits: 0, Misses: 34, Kills: 0\n",
      "  Moves: 2378, TurnL: 24, TurnR: 41, Switch: 23\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 65 Summary ===\n",
      "Player  Reward: -637.70\n",
      "  Hits: 0, Misses: 352, Kills: 0\n",
      "  Moves: 1992, TurnL: 2, TurnR: 48, Switch: 106\n",
      "Opponent Reward: -268.10\n",
      "  Hits: 0, Misses: 2, Kills: 0\n",
      "  Moves: 2450, TurnL: 2, TurnR: 45, Switch: 1\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 66 Summary ===\n",
      "Player  Reward: -545.40\n",
      "  Hits: 0, Misses: 149, Kills: 0\n",
      "  Moves: 1310, TurnL: 555, TurnR: 173, Switch: 313\n",
      "Opponent Reward: -109.00\n",
      "  Hits: 0, Misses: 101, Kills: 0\n",
      "  Moves: 2158, TurnL: 90, TurnR: 73, Switch: 78\n",
      "===================================\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[187], line 501\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUMBER_OF_STEPS):\n\u001b[1;32m    500\u001b[0m     p_ns, p_r, p_done \u001b[38;5;241m=\u001b[39m player_agent\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 501\u001b[0m     o_ns, o_r, o_done \u001b[38;5;241m=\u001b[39m \u001b[43mopponent_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m     p_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m p_r\n\u001b[1;32m    504\u001b[0m     o_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m o_r\n",
      "Cell \u001b[0;32mIn[187], line 346\u001b[0m, in \u001b[0;36mDQNAgent.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m done   \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment_simulator\u001b[38;5;241m.\u001b[39mis_terminal()\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39madd(state, action, reward, next_state, done)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# 更新前一動作與位置\u001b[39;00m\n\u001b[1;32m    348\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment_simulator\n",
      "Cell \u001b[0;32mIn[187], line 286\u001b[0m, in \u001b[0;36mDQNAgent.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size: \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    285\u001b[0m s,a,r,ns,d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m--> 286\u001b[0m q_vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, a\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    288\u001b[0m     max_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_network(ns)\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/CSC584/Project/Deadzone/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/CSC584/Project/Deadzone/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[187], line 200\u001b[0m, in \u001b[0;36mQNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    199\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[0;32m--> 200\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n",
      "File \u001b[0;32m~/Desktop/CSC584/Project/Deadzone/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/CSC584/Project/Deadzone/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/CSC584/Project/Deadzone/venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ----------------------------\n",
    "# Environment Simulator\n",
    "# ----------------------------\n",
    "class EnvironmentSimulator:\n",
    "    def __init__(self):\n",
    "        self.map = [\n",
    "            [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "            [1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "            [1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "            [1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "            [1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1],\n",
    "            [1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,1,0,0,1],\n",
    "            [1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1],\n",
    "            [1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1],\n",
    "            [1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1],\n",
    "            [1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1],\n",
    "            [1,0,0,0,0,1,1,1,0,0,1,0,0,0,1,0,0,0,0,0,1],\n",
    "            [1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1],\n",
    "            [1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1],\n",
    "            [1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1],\n",
    "            [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "        ]\n",
    "        self.player_gun = random.randint(0,1)\n",
    "        self.player_pos = self.initialize_position()\n",
    "        self.player_orientation = random.randint(-180,180)\n",
    "        self.player_health = 10\n",
    "\n",
    "        self.opponent_gun = random.randint(0,1)\n",
    "        self.opponent_pos = self.initialize_position()\n",
    "        self.opponent_orientation = random.randint(-180,180)\n",
    "        self.opponent_health = 10\n",
    "\n",
    "    def initialize_position(self):\n",
    "        pos = (random.randint(1,13), random.randint(1,18))\n",
    "        while self.map[pos[0]][pos[1]] == 1:\n",
    "            pos = (random.randint(1,13), random.randint(1,18))\n",
    "        return [pos[0], pos[1]]\n",
    "\n",
    "    def move(self, offset, movePlayer=True):\n",
    "        if movePlayer:\n",
    "            nr = self.player_pos[0] + offset[0]\n",
    "            nc = self.player_pos[1] + offset[1]\n",
    "            if self.map[nr][nc] != 1:\n",
    "                self.player_pos = [nr,nc]\n",
    "        else:\n",
    "            nr = self.opponent_pos[0] + offset[0]\n",
    "            nc = self.opponent_pos[1] + offset[1]\n",
    "            if self.map[nr][nc] != 1:\n",
    "                self.opponent_pos = [nr,nc]\n",
    "\n",
    "    def turn(self, turnLeft=True, turnPlayer=True):\n",
    "        if turnPlayer:\n",
    "            self.player_orientation += 5 if turnLeft else -5\n",
    "            self.player_orientation = self.mapToRange(self.player_orientation)\n",
    "        else:\n",
    "            self.opponent_orientation += 5 if turnLeft else -5\n",
    "            self.opponent_orientation = self.mapToRange(self.opponent_orientation)\n",
    "\n",
    "    def mapToRange(self, deg):\n",
    "        deg %= 360\n",
    "        if deg > 180: deg -= 360\n",
    "        return deg\n",
    "\n",
    "    def switchWeapon(self, switchPlayerWeapon=True):\n",
    "        if switchPlayerWeapon:\n",
    "            self.player_gun = not self.player_gun\n",
    "        else:\n",
    "            self.opponent_gun = not self.opponent_gun\n",
    "\n",
    "    def shoot(self, playerShooting=True):\n",
    "        if playerShooting:\n",
    "            ang = math.degrees(math.atan2(\n",
    "                self.opponent_pos[0]-self.player_pos[0],\n",
    "                self.opponent_pos[1]-self.player_pos[1]))\n",
    "            diff = ang - self.player_orientation\n",
    "            dist = math.hypot(\n",
    "                self.opponent_pos[0]-self.player_pos[0],\n",
    "                self.opponent_pos[1]-self.player_pos[1])\n",
    "            if self.player_gun == 1:\n",
    "                if abs(diff)<10 and not self.has_obstacle_between() and dist*32<=300:\n",
    "                    self.opponent_health -= 1\n",
    "                    return True\n",
    "            else:\n",
    "                if abs(diff)<10 and not self.has_obstacle_between() and dist*32<=100:\n",
    "                    self.opponent_health -= 3\n",
    "                    return True\n",
    "        else:\n",
    "            ang = math.degrees(math.atan2(\n",
    "                self.player_pos[0]-self.opponent_pos[0],\n",
    "                self.player_pos[1]-self.opponent_pos[1]))\n",
    "            diff = ang - self.opponent_orientation\n",
    "            dist = math.hypot(\n",
    "                self.player_pos[0]-self.opponent_pos[0],\n",
    "                self.player_pos[1]-self.opponent_pos[1])\n",
    "            if self.opponent_gun == 1:\n",
    "                if abs(diff)<10 and not self.has_obstacle_between() and dist*32<=300:\n",
    "                    self.player_health -= 1\n",
    "                    return True\n",
    "            else:\n",
    "                if abs(diff)<10 and not self.has_obstacle_between() and dist*32<=100:\n",
    "                    self.player_health -= 3\n",
    "                    return True\n",
    "\n",
    "    def perform(self, action, movePlayer=True):\n",
    "        # perform on player or opponent\n",
    "        if action == 0:\n",
    "            self.shoot(movePlayer)\n",
    "        elif action == 1:\n",
    "            self.move((-1, 0), movePlayer)\n",
    "        elif action == 2:\n",
    "            self.move((1, 0), movePlayer)\n",
    "        elif action == 3:\n",
    "            self.move((0, -1), movePlayer)\n",
    "        elif action == 4:\n",
    "            self.move((0, 1), movePlayer)\n",
    "        elif action == 5:\n",
    "            self.move((-1, -1), movePlayer)\n",
    "        elif action == 6:\n",
    "            self.move((-1, 1), movePlayer)\n",
    "        elif action == 7:\n",
    "            self.move((1, -1), movePlayer)\n",
    "        elif action == 8:\n",
    "            self.move((1, 1), movePlayer)\n",
    "        elif action == 9:\n",
    "            self.turn(True, movePlayer)\n",
    "        elif action == 10:\n",
    "            self.turn(False, movePlayer)\n",
    "        elif action == 11:\n",
    "            self.switchWeapon(movePlayer)\n",
    "\n",
    "    def ray_distance(self, angle_deg, max_dist=None):\n",
    "        if max_dist is None:\n",
    "            max_dist = max(len(self.map), len(self.map[0]))\n",
    "        rad = math.radians(angle_deg)\n",
    "        dx, dy = math.cos(rad), math.sin(rad)\n",
    "        px, py = self.player_pos[1], self.player_pos[0]\n",
    "        for d in range(1, max_dist+1):\n",
    "            x = int(round(px + dx*d))\n",
    "            y = int(round(py + dy*d))\n",
    "            if not (0 <= y < len(self.map) and 0 <= x < len(self.map[0])):\n",
    "                return d\n",
    "            if self.map[y][x] == 1:\n",
    "                return d\n",
    "        return max_dist\n",
    "\n",
    "    def bresenham_line(self, x0, y0, x1, y1):\n",
    "        pts = []\n",
    "        dx = abs(x1-x0); dy = -abs(y1-y0)\n",
    "        sx = 1 if x0<x1 else -1; sy = 1 if y0<y1 else -1\n",
    "        err = dx+dy\n",
    "        while True:\n",
    "            pts.append((x0,y0))\n",
    "            if x0==x1 and y0==y1: break\n",
    "            e2 = 2*err\n",
    "            if e2 >= dy: err += dy; x0 += sx\n",
    "            if e2 <= dx: err += dx; y0 += sy\n",
    "        return pts\n",
    "\n",
    "    def has_obstacle_between(self):\n",
    "        line = self.bresenham_line(\n",
    "            self.player_pos[1], self.player_pos[0],\n",
    "            self.opponent_pos[1], self.opponent_pos[0])\n",
    "        for x,y in line[1:-1]:\n",
    "            if self.map[y][x] == 1: return True\n",
    "        return False\n",
    "\n",
    "    def is_terminal(self):\n",
    "        return self.player_health == 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.player_gun = random.randint(0,1)\n",
    "        self.player_pos = self.initialize_position()\n",
    "        self.player_orientation = random.randint(-180,180)\n",
    "        self.player_health = 10\n",
    "        self.opponent_gun = random.randint(0,1)\n",
    "        self.opponent_pos = self.initialize_position()\n",
    "        self.opponent_orientation = random.randint(-180,180)\n",
    "        self.opponent_health = 10\n",
    "\n",
    "# ----------------------------\n",
    "# Q-Network & Replay Buffer\n",
    "# ----------------------------\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=256):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def add(self, s,a,r,ns,d):\n",
    "        self.buffer.append((s,a,r,ns,d))\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        s,a,r,ns,d = map(np.array, zip(*batch))\n",
    "        return (torch.FloatTensor(s),\n",
    "                torch.LongTensor(a),\n",
    "                torch.FloatTensor(r),\n",
    "                torch.FloatTensor(ns),\n",
    "                torch.FloatTensor(d))\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# ----------------------------\n",
    "# DQN Agent with Metrics\n",
    "# ----------------------------\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size,\n",
    "                 lr=1e-3, gamma=0.99,\n",
    "                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995,\n",
    "                 target_update_freq=10, tau=None, opponent=False):\n",
    "        self.state_size    = state_size\n",
    "        self.action_size   = action_size\n",
    "        self.q_network     = QNetwork(state_size, action_size)\n",
    "        self.target_network= QNetwork(state_size, action_size)\n",
    "        self.optimizer     = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.replay_buffer = ReplayBuffer(10000)\n",
    "        self.batch_size    = 64\n",
    "        self.gamma         = gamma\n",
    "        self.environment_simulator = EnvironmentSimulator()\n",
    "        self.player        = not opponent\n",
    "        self.prev_action = None\n",
    "        self.prev_position = None\n",
    "\n",
    "        # epsilon-greedy\n",
    "        self.epsilon       = epsilon_start\n",
    "        self.epsilon_end   = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # target update\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.tau           = tau\n",
    "        self._train_steps  = 0\n",
    "\n",
    "        # Metrics\n",
    "        self.hit_count        = 0\n",
    "        self.miss_count       = 0\n",
    "        self.kill_count       = 0\n",
    "        self.move_count       = 0\n",
    "        self.turn_left_count  = 0\n",
    "        self.turn_right_count = 0\n",
    "        self.switch_count     = 0\n",
    "\n",
    "        self.update_target()\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        self.hit_count        = 0\n",
    "        self.miss_count       = 0\n",
    "        self.kill_count       = 0\n",
    "        self.move_count       = 0\n",
    "        self.turn_left_count  = 0\n",
    "        self.turn_right_count = 0\n",
    "        self.switch_count     = 0\n",
    "        self.prev_action = None\n",
    "        self.prev_position = None\n",
    "\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_size-1)\n",
    "        with torch.no_grad():\n",
    "            s = torch.FloatTensor(state).unsqueeze(0)\n",
    "            return self.q_network(s).argmax().item()\n",
    "\n",
    "    def train_step(self):\n",
    "        if len(self.replay_buffer) < self.batch_size: return\n",
    "        s,a,r,ns,d = self.replay_buffer.sample(self.batch_size)\n",
    "        q_vals = self.q_network(s).gather(1, a.unsqueeze(1)).squeeze(1)\n",
    "        with torch.no_grad():\n",
    "            max_next = self.target_network(ns).max(1)[0]\n",
    "            target_q = r + (1-d)*self.gamma*max_next\n",
    "        loss = nn.MSELoss()(q_vals, target_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # epsilon decay\n",
    "        self.epsilon = max(self.epsilon*self.epsilon_decay, self.epsilon_end)\n",
    "        # target update\n",
    "        if self.tau is None:\n",
    "            self._train_steps += 1\n",
    "            if self._train_steps % self.target_update_freq == 0:\n",
    "                self.update_target()\n",
    "        else:\n",
    "            for tp,p in zip(self.target_network.parameters(),\n",
    "                            self.q_network.parameters()):\n",
    "                tp.data.copy_(self.tau*p.data + (1-self.tau)*tp.data)\n",
    "\n",
    "    def step(self):\n",
    "        state  = self.getState()\n",
    "        action = self.select_action(state)\n",
    "\n",
    "        # record HP before shoot\n",
    "        prev_hp = (self.environment_simulator.opponent_health\n",
    "                   if self.player else\n",
    "                   self.environment_simulator.player_health)\n",
    "\n",
    "        # perform with correct actor\n",
    "        self.environment_simulator.perform(action, movePlayer=self.player)\n",
    "        next_state = self.getState()\n",
    "\n",
    "        # record HP after\n",
    "        new_hp = (self.environment_simulator.opponent_health\n",
    "                  if self.player else\n",
    "                  self.environment_simulator.player_health)\n",
    "\n",
    "        # metrics: shoot\n",
    "        if action == 0:\n",
    "            if new_hp < prev_hp:\n",
    "                self.hit_count += 1\n",
    "                if new_hp <= 0:\n",
    "                    self.kill_count += 1\n",
    "            else:\n",
    "                self.miss_count += 1\n",
    "        # metrics: other\n",
    "        if action in range(1,9):\n",
    "            self.move_count += 1\n",
    "        elif action == 9:\n",
    "            self.turn_left_count += 1\n",
    "        elif action == 10:\n",
    "            self.turn_right_count += 1\n",
    "        elif action == 11:\n",
    "            self.switch_count += 1\n",
    "\n",
    "        reward = self.calculate_reward(state, action, next_state)\n",
    "        done   = self.environment_simulator.is_terminal()\n",
    "        self.replay_buffer.add(state, action, reward, next_state, done)\n",
    "        self.train_step()\n",
    "        # 更新前一動作與位置\n",
    "        env = self.environment_simulator\n",
    "        self.prev_action = action\n",
    "        self.prev_position = env.player_pos[:] if self.player else env.opponent_pos[:]\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def calculate_reward(self, state, action, next_state):\n",
    "        \"\"\"\n",
    "        強化探索 + 正確進攻 的獎勵函式\n",
    "        -------------------------------------------------\n",
    "        state, next_state : 11 維向量（詳見 getState()）\n",
    "        action            : 0~11\n",
    "        -------------------------------------------------\n",
    "        \"\"\"\n",
    "\n",
    "        # ── 基本幾何量 ─────────────────────────────────\n",
    "        # 敵人距離（tile 為單位）\n",
    "        H, W = len(self.environment_simulator.map), len(self.environment_simulator.map[0])\n",
    "        dist_prev = math.hypot(state[2] * H,  state[3] * W)\n",
    "        dist_next = math.hypot(next_state[2] * H, next_state[3] * W)\n",
    "\n",
    "        # 角度差（敵人相對於玩家視線）\n",
    "        ori_deg = (self.environment_simulator.player_orientation % 360)\n",
    "        angle_to_enemy = self.angle_between_player_and_opponent((state[2], state[3]))\n",
    "        ang_diff = self.angleDiff(angle_to_enemy, ori_deg)\n",
    "\n",
    "        # 是否在 ±30° 內、且无遮挡\n",
    "        in_fov  = abs(ang_diff) <= 30\n",
    "        isClear = not self.environment_simulator.has_obstacle_between()\n",
    "        enemy_visible = in_fov and isClear\n",
    "\n",
    "        # ── reward 初始化 ─────────────────────────────\n",
    "        reward = 0.0\n",
    "\n",
    "        # ── 亂射 or 命中/擊殺 ────────────────────────\n",
    "        opp_hp_prev = state[4]\n",
    "        opp_hp_next = next_state[4]\n",
    "        hit   = (opp_hp_next < opp_hp_prev)\n",
    "        kill  = (opp_hp_next <= 0)\n",
    "\n",
    "        if action == 0:                           # 射擊\n",
    "            if enemy_visible and hit:\n",
    "                reward += 200.0                   # 命中\n",
    "                if kill:\n",
    "                    reward += 1000.0              # 擊殺\n",
    "            elif enemy_visible and not hit:\n",
    "                reward -= 5.0                     # 可見卻沒打中\n",
    "            else:\n",
    "                reward -= 2.0                     # 看不到還亂射\n",
    "        # ── 移動探索 ─────────────────────────────────\n",
    "        move_actions = range(1, 9)\n",
    "        turn_left, turn_right = 9, 10\n",
    "\n",
    "        # 當前座標\n",
    "        curr_pos = self.environment_simulator.player_pos if self.player \\\n",
    "                else self.environment_simulator.opponent_pos\n",
    "\n",
    "        # 1) 有效移動\n",
    "        if action in move_actions:\n",
    "            if self.prev_position and curr_pos != self.prev_position:\n",
    "                reward += 1.0                     # 位置真的改變\n",
    "            else:\n",
    "                reward -= 0.5                     # 撞牆或卡位\n",
    "            # 若敵人可見且距離變近，額外鼓勵\n",
    "            if enemy_visible and dist_next < dist_prev:\n",
    "                reward += 0.3\n",
    "\n",
    "        # 2) 轉向\n",
    "        if action in (turn_left, turn_right):\n",
    "            # 若敵人不可見 → 探索視野\n",
    "            if not enemy_visible:\n",
    "                reward += 0.3\n",
    "            else:\n",
    "                # 若可見 → 判斷是否朝正確方向微調\n",
    "                if (action == turn_left  and ang_diff > 0) or \\\n",
    "                (action == turn_right and ang_diff < 0):\n",
    "                    reward += 0.5                # 角度誤差有望減小\n",
    "                else:\n",
    "                    reward -= 0.2                # 轉錯方向\n",
    "\n",
    "        # 3) 武器切換（保持簡單：距離<4 tile 用散彈，>4 tile 用步槍）\n",
    "        if action == 11:\n",
    "            use_rifle = bool(state[6])\n",
    "            close_range = dist_prev < 4.0\n",
    "            if (close_range and use_rifle) or (not close_range and not use_rifle):\n",
    "                reward += 1.0                    # 切對武器\n",
    "            else:\n",
    "                reward -= 0.5                    # 切錯武器\n",
    "\n",
    "        # ── 動作多樣性 ────────────────────────────────\n",
    "        if self.prev_action is not None and action != self.prev_action:\n",
    "            reward += 0.2\n",
    "\n",
    "        # ── 存活懲罰（可選）──────────────────────────\n",
    "        # 若自己 HP 下降，可給小懲罰；此處略。\n",
    "\n",
    "        return reward\n",
    "\n",
    "\n",
    "    def getState(self):\n",
    "        env = self.environment_simulator\n",
    "        H, W = len(env.map), len(env.map[0])\n",
    "        pr, pc = env.player_pos\n",
    "        row_norm, col_norm = pr/H, pc/W\n",
    "        or_, oc = env.opponent_pos\n",
    "        d_row, d_col = (or_-pr)/H, (oc-pc)/W\n",
    "        opp_hp, ply_hp = env.opponent_health/10.0, env.player_health/10.0\n",
    "        gun = float(env.player_gun)\n",
    "        ori = (env.player_orientation % 360)/360.0\n",
    "        def ray(a): return env.ray_distance(a, 10)/10.0\n",
    "        front  = ray(ori*360)\n",
    "        left30 = ray((ori*360+30)%360)\n",
    "        right30= ray((ori*360-30)%360)\n",
    "        return [row_norm, col_norm, d_row, d_col, opp_hp, ply_hp, gun, ori, front, left30, right30]\n",
    "\n",
    "    def magnitude(self, v):\n",
    "        return math.hypot(v[0], v[1])\n",
    "\n",
    "    def mapToRange(self, deg):\n",
    "        deg %= 360\n",
    "        if deg>180: deg -= 360\n",
    "        return deg\n",
    "\n",
    "    def angle_between_player_and_opponent(self, rel):\n",
    "        return self.mapToRange(math.degrees(math.atan2(rel[0], rel[1])))\n",
    "\n",
    "    def angleDiff(self, a1, a2):\n",
    "        d = a1 - a2\n",
    "        if d>180: d -= 360\n",
    "        if d<-180: d += 360\n",
    "        return d\n",
    "\n",
    "    def save_model(self, filename=\"dqn_model.pth\"):\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "        torch.save(self.q_network.state_dict(), os.path.join(\"models\", filename))\n",
    "\n",
    "    def load_model(self, filename=\"dqn_model.pth\"):\n",
    "        self.q_network.load_state_dict(torch.load(os.path.join(\"models\", filename)))\n",
    "        self.q_network.eval()\n",
    "\n",
    "# ----------------------------\n",
    "# Training Loop\n",
    "# ----------------------------\n",
    "NUMBER_OF_EPISODES = 2000\n",
    "NUMBER_OF_STEPS    = 2500\n",
    "\n",
    "player_agent   = DQNAgent(11, 12)\n",
    "opponent_agent = DQNAgent(11, 12, opponent=True)\n",
    "\n",
    "def reset():\n",
    "    player_agent.environment_simulator.reset()\n",
    "    es = player_agent.environment_simulator\n",
    "    osim = opponent_agent.environment_simulator\n",
    "    osim.player_pos         = es.opponent_pos[:]\n",
    "    osim.player_health      = es.opponent_health\n",
    "    osim.player_orientation = es.opponent_orientation\n",
    "    osim.player_gun         = es.opponent_gun\n",
    "    osim.opponent_pos       = es.player_pos[:]\n",
    "    osim.opponent_health    = es.player_health\n",
    "    osim.opponent_orientation = es.player_orientation\n",
    "    osim.opponent_gun       = es.player_gun\n",
    "\n",
    "for ep in range(NUMBER_OF_EPISODES):\n",
    "    player_agent.reset_metrics()\n",
    "    opponent_agent.reset_metrics()\n",
    "    p_total, o_total = 0.0, 0.0\n",
    "    p_done, o_done = False, False\n",
    "\n",
    "    for step in range(NUMBER_OF_STEPS):\n",
    "        p_ns, p_r, p_done = player_agent.step()\n",
    "        o_ns, o_r, o_done = opponent_agent.step()\n",
    "\n",
    "        p_total += p_r\n",
    "        o_total += o_r\n",
    "\n",
    "        if p_done or o_done:\n",
    "            break\n",
    "\n",
    "        # 同步環境\n",
    "        es = player_agent.environment_simulator\n",
    "        osim = opponent_agent.environment_simulator\n",
    "        osim.player_pos         = es.opponent_pos[:]\n",
    "        osim.player_health      = es.opponent_health\n",
    "        osim.player_orientation = es.opponent_orientation\n",
    "        osim.player_gun         = es.opponent_gun\n",
    "        osim.opponent_pos       = es.player_pos[:]\n",
    "        osim.opponent_health    = es.player_health\n",
    "        osim.opponent_orientation = es.player_orientation\n",
    "        osim.opponent_gun       = es.player_gun\n",
    "\n",
    "    # Episode 統計\n",
    "    print(f\"\\n=== Episode {ep} Summary ===\")\n",
    "    print(f\"Player  Reward: {p_total:.2f}\")\n",
    "    print(f\"  Hits: {player_agent.hit_count}, Misses: {player_agent.miss_count}, Kills: {player_agent.kill_count}\")\n",
    "    print(f\"  Moves: {player_agent.move_count}, TurnL: {player_agent.turn_left_count}, TurnR: {player_agent.turn_right_count}, Switch: {player_agent.switch_count}\")\n",
    "    print(f\"Opponent Reward: {o_total:.2f}\")\n",
    "    print(f\"  Hits: {opponent_agent.hit_count}, Misses: {opponent_agent.miss_count}, Kills: {opponent_agent.kill_count}\")\n",
    "    print(f\"  Moves: {opponent_agent.move_count}, TurnL: {opponent_agent.turn_left_count}, TurnR: {opponent_agent.turn_right_count}, Switch: {opponent_agent.switch_count}\")\n",
    "    print(\"===================================\\n\")\n",
    "\n",
    "    # 決定勝者並存檔權重\n",
    "    if p_done:\n",
    "        winner = opponent_agent\n",
    "        wname = 'opponent'\n",
    "    elif o_done:\n",
    "        winner = player_agent\n",
    "        wname = 'player'\n",
    "    else:\n",
    "        if p_total >= o_total:\n",
    "            winner = player_agent\n",
    "            wname = 'player'\n",
    "        else:\n",
    "            winner = opponent_agent\n",
    "            wname = 'opponent'\n",
    "    winner.save_model(f\"{wname}_winner_ep{ep}.pth\")\n",
    "    # 載入勝者權重給雙方\n",
    "    state_dict = winner.q_network.state_dict()\n",
    "    player_agent.q_network.load_state_dict(state_dict)\n",
    "    opponent_agent.q_network.load_state_dict(state_dict)\n",
    "    player_agent.update_target()\n",
    "    opponent_agent.update_target()\n",
    "\n",
    "    reset()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
