{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Episode 0 Summary ===\n",
      "Player  Reward: -5665.50\n",
      "  Hits: 10, Misses: 543, Kills: 6\n",
      "  Moves: 1511, TurnL: 138, TurnR: 221, Switch: 77\n",
      "Opponent Reward: -9365.90\n",
      "  Hits: 1, Misses: 21, Kills: 0\n",
      "  Moves: 1213, TurnL: 91, TurnR: 99, Switch: 1075\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 1 Summary ===\n",
      "Player  Reward: -13146.60\n",
      "  Hits: 0, Misses: 718, Kills: 0\n",
      "  Moves: 1077, TurnL: 56, TurnR: 304, Switch: 345\n",
      "Opponent Reward: -12337.60\n",
      "  Hits: 0, Misses: 28, Kills: 0\n",
      "  Moves: 2130, TurnL: 89, TurnR: 51, Switch: 202\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 2 Summary ===\n",
      "Player  Reward: -12474.50\n",
      "  Hits: 0, Misses: 284, Kills: 0\n",
      "  Moves: 1473, TurnL: 258, TurnR: 284, Switch: 201\n",
      "Opponent Reward: -12890.30\n",
      "  Hits: 0, Misses: 1, Kills: 0\n",
      "  Moves: 2028, TurnL: 11, TurnR: 2, Switch: 458\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 3 Summary ===\n",
      "Player  Reward: -12616.90\n",
      "  Hits: 0, Misses: 310, Kills: 0\n",
      "  Moves: 1579, TurnL: 254, TurnR: 140, Switch: 217\n",
      "Opponent Reward: -12858.70\n",
      "  Hits: 0, Misses: 3, Kills: 0\n",
      "  Moves: 2493, TurnL: 4, TurnR: 0, Switch: 0\n",
      "===================================\n",
      "\n",
      "\n",
      "=== Episode 4 Summary ===\n",
      "Player  Reward: -12260.30\n",
      "  Hits: 0, Misses: 219, Kills: 0\n",
      "  Moves: 1307, TurnL: 129, TurnR: 543, Switch: 302\n",
      "Opponent Reward: -12774.40\n",
      "  Hits: 0, Misses: 1, Kills: 0\n",
      "  Moves: 2451, TurnL: 3, TurnR: 2, Switch: 43\n",
      "===================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ----------------------------\n",
    "# Environment Simulator\n",
    "# ----------------------------\n",
    "class EnvironmentSimulator:\n",
    "    def __init__(self):\n",
    "        self.map = [\n",
    "            [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "            [1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "            [1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "            [1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "            [1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1],\n",
    "            [1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,1,1,0,0,1],\n",
    "            [1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1],\n",
    "            [1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1],\n",
    "            [1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1],\n",
    "            [1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1],\n",
    "            [1,0,0,0,0,1,1,1,0,0,1,0,0,0,1,0,0,0,0,0,1],\n",
    "            [1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1],\n",
    "            [1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1],\n",
    "            [1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1],\n",
    "            [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "        ]\n",
    "        self.player_gun = random.randint(0,1)\n",
    "        self.player_pos = self.initialize_position()\n",
    "        self.player_orientation = random.randint(-180,180)\n",
    "        self.player_health = 10\n",
    "\n",
    "        self.opponent_gun = random.randint(0,1)\n",
    "        self.opponent_pos = self.initialize_position()\n",
    "        self.opponent_orientation = random.randint(-180,180)\n",
    "        self.opponent_health = 10\n",
    "\n",
    "    def initialize_position(self):\n",
    "        pos = (random.randint(1,13), random.randint(1,18))\n",
    "        while self.map[pos[0]][pos[1]] == 1:\n",
    "            pos = (random.randint(1,13), random.randint(1,18))\n",
    "        return [pos[0], pos[1]]\n",
    "\n",
    "    def move(self, offset, movePlayer=True):\n",
    "        if movePlayer:\n",
    "            nr = self.player_pos[0] + offset[0]\n",
    "            nc = self.player_pos[1] + offset[1]\n",
    "            if self.map[nr][nc] != 1:\n",
    "                self.player_pos = [nr,nc]\n",
    "        else:\n",
    "            nr = self.opponent_pos[0] + offset[0]\n",
    "            nc = self.opponent_pos[1] + offset[1]\n",
    "            if self.map[nr][nc] != 1:\n",
    "                self.opponent_pos = [nr,nc]\n",
    "\n",
    "    def turn(self, turnLeft=True, turnPlayer=True):\n",
    "        if turnPlayer:\n",
    "            self.player_orientation += 5 if turnLeft else -5\n",
    "            self.player_orientation = self.mapToRange(self.player_orientation)\n",
    "        else:\n",
    "            self.opponent_orientation += 5 if turnLeft else -5\n",
    "            self.opponent_orientation = self.mapToRange(self.opponent_orientation)\n",
    "\n",
    "    def mapToRange(self, deg):\n",
    "        deg %= 360\n",
    "        if deg > 180: deg -= 360\n",
    "        return deg\n",
    "\n",
    "    def switchWeapon(self, switchPlayerWeapon=True):\n",
    "        if switchPlayerWeapon:\n",
    "            self.player_gun = not self.player_gun\n",
    "        else:\n",
    "            self.opponent_gun = not self.opponent_gun\n",
    "\n",
    "    def shoot(self, playerShooting=True):\n",
    "        if playerShooting:\n",
    "            ang = math.degrees(math.atan2(\n",
    "                self.opponent_pos[0]-self.player_pos[0],\n",
    "                self.opponent_pos[1]-self.player_pos[1]))\n",
    "            diff = ang - self.player_orientation\n",
    "            dist = math.hypot(\n",
    "                self.opponent_pos[0]-self.player_pos[0],\n",
    "                self.opponent_pos[1]-self.player_pos[1])\n",
    "            if self.player_gun == 1:\n",
    "                if abs(diff)<10 and not self.has_obstacle_between() and dist*32<=300:\n",
    "                    self.opponent_health -= 1\n",
    "                    return True\n",
    "            else:\n",
    "                if abs(diff)<10 and not self.has_obstacle_between() and dist*32<=100:\n",
    "                    self.opponent_health -= 3\n",
    "                    return True\n",
    "        else:\n",
    "            ang = math.degrees(math.atan2(\n",
    "                self.player_pos[0]-self.opponent_pos[0],\n",
    "                self.player_pos[1]-self.opponent_pos[1]))\n",
    "            diff = ang - self.opponent_orientation\n",
    "            dist = math.hypot(\n",
    "                self.player_pos[0]-self.opponent_pos[0],\n",
    "                self.player_pos[1]-self.opponent_pos[1])\n",
    "            if self.opponent_gun == 1:\n",
    "                if abs(diff)<10 and not self.has_obstacle_between() and dist*32<=300:\n",
    "                    self.player_health -= 1\n",
    "                    return True\n",
    "            else:\n",
    "                if abs(diff)<10 and not self.has_obstacle_between() and dist*32<=100:\n",
    "                    self.player_health -= 3\n",
    "                    return True\n",
    "\n",
    "    def perform(self, action, movePlayer=True):\n",
    "        # perform on player or opponent\n",
    "        if action == 0:\n",
    "            self.shoot(movePlayer)\n",
    "        elif action == 1:\n",
    "            self.move((-1, 0), movePlayer)\n",
    "        elif action == 2:\n",
    "            self.move((1, 0), movePlayer)\n",
    "        elif action == 3:\n",
    "            self.move((0, -1), movePlayer)\n",
    "        elif action == 4:\n",
    "            self.move((0, 1), movePlayer)\n",
    "        elif action == 5:\n",
    "            self.move((-1, -1), movePlayer)\n",
    "        elif action == 6:\n",
    "            self.move((-1, 1), movePlayer)\n",
    "        elif action == 7:\n",
    "            self.move((1, -1), movePlayer)\n",
    "        elif action == 8:\n",
    "            self.move((1, 1), movePlayer)\n",
    "        elif action == 9:\n",
    "            self.turn(True, movePlayer)\n",
    "        elif action == 10:\n",
    "            self.turn(False, movePlayer)\n",
    "        elif action == 11:\n",
    "            self.switchWeapon(movePlayer)\n",
    "\n",
    "    def ray_distance(self, angle_deg, max_dist=None):\n",
    "        if max_dist is None:\n",
    "            max_dist = max(len(self.map), len(self.map[0]))\n",
    "        rad = math.radians(angle_deg)\n",
    "        dx, dy = math.cos(rad), math.sin(rad)\n",
    "        px, py = self.player_pos[1], self.player_pos[0]\n",
    "        for d in range(1, max_dist+1):\n",
    "            x = int(round(px + dx*d))\n",
    "            y = int(round(py + dy*d))\n",
    "            if not (0 <= y < len(self.map) and 0 <= x < len(self.map[0])):\n",
    "                return d\n",
    "            if self.map[y][x] == 1:\n",
    "                return d\n",
    "        return max_dist\n",
    "\n",
    "    def bresenham_line(self, x0, y0, x1, y1):\n",
    "        pts = []\n",
    "        dx = abs(x1-x0); dy = -abs(y1-y0)\n",
    "        sx = 1 if x0<x1 else -1; sy = 1 if y0<y1 else -1\n",
    "        err = dx+dy\n",
    "        while True:\n",
    "            pts.append((x0,y0))\n",
    "            if x0==x1 and y0==y1: break\n",
    "            e2 = 2*err\n",
    "            if e2 >= dy: err += dy; x0 += sx\n",
    "            if e2 <= dx: err += dx; y0 += sy\n",
    "        return pts\n",
    "\n",
    "    def has_obstacle_between(self):\n",
    "        line = self.bresenham_line(\n",
    "            self.player_pos[1], self.player_pos[0],\n",
    "            self.opponent_pos[1], self.opponent_pos[0])\n",
    "        for x,y in line[1:-1]:\n",
    "            if self.map[y][x] == 1: return True\n",
    "        return False\n",
    "\n",
    "    def is_terminal(self):\n",
    "        return self.player_health == 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.player_gun = random.randint(0,1)\n",
    "        self.player_pos = self.initialize_position()\n",
    "        self.player_orientation = random.randint(-180,180)\n",
    "        self.player_health = 10\n",
    "        self.opponent_gun = random.randint(0,1)\n",
    "        self.opponent_pos = self.initialize_position()\n",
    "        self.opponent_orientation = random.randint(-180,180)\n",
    "        self.opponent_health = 10\n",
    "\n",
    "# ----------------------------\n",
    "# Q-Network & Replay Buffer\n",
    "# ----------------------------\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=256):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def add(self, s,a,r,ns,d):\n",
    "        self.buffer.append((s,a,r,ns,d))\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        s,a,r,ns,d = map(np.array, zip(*batch))\n",
    "        return (torch.FloatTensor(s),\n",
    "                torch.LongTensor(a),\n",
    "                torch.FloatTensor(r),\n",
    "                torch.FloatTensor(ns),\n",
    "                torch.FloatTensor(d))\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# ----------------------------\n",
    "# DQN Agent with Metrics\n",
    "# ----------------------------\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size,\n",
    "                 lr=1e-3, gamma=0.99,\n",
    "                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995,\n",
    "                 target_update_freq=10, tau=None, opponent=False):\n",
    "        self.state_size    = state_size\n",
    "        self.action_size   = action_size\n",
    "        self.q_network     = QNetwork(state_size, action_size)\n",
    "        self.target_network= QNetwork(state_size, action_size)\n",
    "        self.optimizer     = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.replay_buffer = ReplayBuffer(10000)\n",
    "        self.batch_size    = 64\n",
    "        self.gamma         = gamma\n",
    "        self.environment_simulator = EnvironmentSimulator()\n",
    "        self.player        = not opponent\n",
    "        self.prev_action = None\n",
    "        self.prev_position = None\n",
    "        self.prev_dist = None          # 上一步與敵人的距離\n",
    "        self.same_action_run = 0       # 連續同動作次數\n",
    "\n",
    "\n",
    "        # epsilon-greedy\n",
    "        self.epsilon       = epsilon_start\n",
    "        self.epsilon_end   = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # target update\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.tau           = tau\n",
    "        self._train_steps  = 0\n",
    "\n",
    "        # Metrics\n",
    "        self.hit_count        = 0\n",
    "        self.miss_count       = 0\n",
    "        self.kill_count       = 0\n",
    "        self.move_count       = 0\n",
    "        self.turn_left_count  = 0\n",
    "        self.turn_right_count = 0\n",
    "        self.switch_count     = 0\n",
    "\n",
    "        self.update_target()\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        self.hit_count        = 0\n",
    "        self.miss_count       = 0\n",
    "        self.kill_count       = 0\n",
    "        self.move_count       = 0\n",
    "        self.turn_left_count  = 0\n",
    "        self.turn_right_count = 0\n",
    "        self.switch_count     = 0\n",
    "        self.prev_action = None\n",
    "        self.prev_position = None\n",
    "        self.prev_dist = None\n",
    "        self.same_action_run = 0\n",
    "\n",
    "\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_size-1)\n",
    "        with torch.no_grad():\n",
    "            s = torch.FloatTensor(state).unsqueeze(0)\n",
    "            return self.q_network(s).argmax().item()\n",
    "\n",
    "    def train_step(self):\n",
    "        if len(self.replay_buffer) < self.batch_size: return\n",
    "        s,a,r,ns,d = self.replay_buffer.sample(self.batch_size)\n",
    "        q_vals = self.q_network(s).gather(1, a.unsqueeze(1)).squeeze(1)\n",
    "        with torch.no_grad():\n",
    "            max_next = self.target_network(ns).max(1)[0]\n",
    "            target_q = r + (1-d)*self.gamma*max_next\n",
    "        loss = nn.MSELoss()(q_vals, target_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # epsilon decay\n",
    "        self.epsilon = max(self.epsilon*self.epsilon_decay, self.epsilon_end)\n",
    "        # target update\n",
    "        if self.tau is None:\n",
    "            self._train_steps += 1\n",
    "            if self._train_steps % self.target_update_freq == 0:\n",
    "                self.update_target()\n",
    "        else:\n",
    "            for tp,p in zip(self.target_network.parameters(),\n",
    "                            self.q_network.parameters()):\n",
    "                tp.data.copy_(self.tau*p.data + (1-self.tau)*tp.data)\n",
    "\n",
    "    def step(self):\n",
    "        state  = self.getState()\n",
    "        action = self.select_action(state)\n",
    "\n",
    "        # record HP before shoot\n",
    "        prev_hp = (self.environment_simulator.opponent_health\n",
    "                   if self.player else\n",
    "                   self.environment_simulator.player_health)\n",
    "\n",
    "        # perform with correct actor\n",
    "        self.environment_simulator.perform(action, movePlayer=self.player)\n",
    "        next_state = self.getState()\n",
    "\n",
    "        # record HP after\n",
    "        new_hp = (self.environment_simulator.opponent_health\n",
    "                  if self.player else\n",
    "                  self.environment_simulator.player_health)\n",
    "\n",
    "        # metrics: shoot\n",
    "        if action == 0:\n",
    "            if new_hp < prev_hp:\n",
    "                self.hit_count += 1\n",
    "                if new_hp <= 0:\n",
    "                    self.kill_count += 1\n",
    "            else:\n",
    "                self.miss_count += 1\n",
    "        # metrics: other\n",
    "        if action in range(1,9):\n",
    "            self.move_count += 1\n",
    "        elif action == 9:\n",
    "            self.turn_left_count += 1\n",
    "        elif action == 10:\n",
    "            self.turn_right_count += 1\n",
    "        elif action == 11:\n",
    "            self.switch_count += 1\n",
    "\n",
    "        reward = self.calculate_reward(state, action, next_state)\n",
    "        done   = self.environment_simulator.is_terminal()\n",
    "        self.replay_buffer.add(state, action, reward, next_state, done)\n",
    "        self.train_step()\n",
    "        # 更新前一動作與位置\n",
    "        # 更新歷史\n",
    "        env = self.environment_simulator\n",
    "        self.prev_action  = action\n",
    "        self.prev_position = env.player_pos[:] if self.player else env.opponent_pos[:]\n",
    "        # 以 tile 為單位距離\n",
    "        self.prev_dist = math.hypot(next_state[2] * len(env.map),\n",
    "                                    next_state[3] * len(env.map[0]))\n",
    "        # 連續同動作統計\n",
    "        if self.same_action_run and action == self.prev_action:\n",
    "            self.same_action_run += 1\n",
    "        else:\n",
    "            self.same_action_run = 1\n",
    "\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def calculate_reward(self, state, action, next_state):\n",
    "        \"\"\"\n",
    "        ──────────────────────────────────────────────────────────────\n",
    "        state[2],state[3]  : 敵人相對 row/col\n",
    "        state[6]           : 1=步槍(遠程) 0=散彈(近程)\n",
    "        state[8]           : 自身朝向 0~1\n",
    "        ray_distance 前/左30/右30 已在 getState() 給出 (state[8]~[10])\n",
    "        ──────────────────────────────────────────────────────────────\n",
    "        \"\"\"\n",
    "        env = self.environment_simulator\n",
    "        H, W = len(env.map), len(env.map[0])\n",
    "\n",
    "        # === 幾何量 ===\n",
    "        dist_prev = math.hypot(state[2]*H,  state[3]*W)\n",
    "        dist_next = math.hypot(next_state[2]*H, next_state[3]*W)\n",
    "\n",
    "        ori_deg = (env.player_orientation % 360)\n",
    "        ang_to_enemy = self.angle_between_player_and_opponent((state[2], state[3]))\n",
    "        ang_diff_prev = abs(self.angleDiff(ang_to_enemy, ori_deg))\n",
    "\n",
    "        # 下一步角度誤差\n",
    "        ang_to_enemy_next = self.angle_between_player_and_opponent((next_state[2], next_state[3]))\n",
    "        ang_diff_next = abs(self.angleDiff(ang_to_enemy_next, ori_deg))\n",
    "\n",
    "        # 可視 + 遮蔽\n",
    "        in_fov = ang_diff_prev <= 30\n",
    "        visible = in_fov and not env.has_obstacle_between()\n",
    "\n",
    "        # === 基礎命中判定 ===\n",
    "        hit  = (next_state[4] < state[4])\n",
    "        kill = (next_state[4] <= 0)\n",
    "\n",
    "        reward = 0.0\n",
    "\n",
    "        # ───────────────── 射擊動作 (0) ─────────────────\n",
    "        if action == 0:\n",
    "            if visible and hit:\n",
    "                reward += 200.0\n",
    "                if kill:\n",
    "                    reward += 1000.0\n",
    "                # combo: 前一步是 turn 而這一步命中\n",
    "                if self.prev_action in (9, 10):\n",
    "                    reward += 50.0\n",
    "            elif visible and not hit:\n",
    "                reward -= 5.0\n",
    "            else:\n",
    "                reward -= 2.0\n",
    "\n",
    "        # ───────────────── 移動動作 (1~8) ───────────────\n",
    "        elif action in range(1, 9):\n",
    "            # 有效移動\n",
    "            curr_pos = env.player_pos if self.player else env.opponent_pos\n",
    "            if self.prev_position and curr_pos != self.prev_position:\n",
    "                reward += 1.0\n",
    "            else:\n",
    "                reward -= 0.5\n",
    "\n",
    "            # 若敵人可見且更接近\n",
    "            if visible and dist_next < dist_prev:\n",
    "                reward += 0.3\n",
    "\n",
    "            # combo: move → turn (若角度改善)\n",
    "            if self.prev_action in range(1, 9) and ang_diff_next < ang_diff_prev:\n",
    "                reward += 0.3\n",
    "\n",
    "        # ───────────────── 轉向動作 (9,10) ──────────────\n",
    "        elif action in (9, 10):\n",
    "            if not visible:\n",
    "                reward += 0.3      # 探索視野\n",
    "            else:\n",
    "                # 若角度誤差縮小\n",
    "                if ang_diff_next < ang_diff_prev:\n",
    "                    reward += 0.5\n",
    "                else:\n",
    "                    reward -= 0.2\n",
    "\n",
    "        # ───────────────── 武器切換 (11) ───────────────\n",
    "        elif action == 11:\n",
    "            close = dist_prev < 4.0\n",
    "            use_rifle = bool(state[6])  # 1=rifle,0=shotgun\n",
    "            correct = (close and not use_rifle) or (not close and use_rifle)\n",
    "            if correct:\n",
    "                reward += 2.0\n",
    "                # combo: move → switchWeapon 當距離剛跨閾值\n",
    "                if self.prev_action in range(1, 9):\n",
    "                    reward += 1.0\n",
    "            else:\n",
    "                reward -= 0.5\n",
    "\n",
    "        # ───────────────── 動作多樣性 & 連續懲罰 ───────\n",
    "        if self.prev_action is not None and action != self.prev_action:\n",
    "            reward += 3\n",
    "        if self.same_action_run >= 3:          # 連續4步同動作\n",
    "            reward -= 5\n",
    "\n",
    "        return reward\n",
    "\n",
    "\n",
    "\n",
    "    def getState(self):\n",
    "        env = self.environment_simulator\n",
    "        H, W = len(env.map), len(env.map[0])\n",
    "        pr, pc = env.player_pos\n",
    "        row_norm, col_norm = pr/H, pc/W\n",
    "        or_, oc = env.opponent_pos\n",
    "        d_row, d_col = (or_-pr)/H, (oc-pc)/W\n",
    "        opp_hp, ply_hp = env.opponent_health/10.0, env.player_health/10.0\n",
    "        gun = float(env.player_gun)\n",
    "        ori = (env.player_orientation % 360)/360.0\n",
    "        def ray(a): return env.ray_distance(a, 10)/10.0\n",
    "        front  = ray(ori*360)\n",
    "        left30 = ray((ori*360+30)%360)\n",
    "        right30= ray((ori*360-30)%360)\n",
    "        return [row_norm, col_norm, d_row, d_col, opp_hp, ply_hp, gun, ori, front, left30, right30]\n",
    "\n",
    "    def magnitude(self, v):\n",
    "        return math.hypot(v[0], v[1])\n",
    "\n",
    "    def mapToRange(self, deg):\n",
    "        deg %= 360\n",
    "        if deg>180: deg -= 360\n",
    "        return deg\n",
    "\n",
    "    def angle_between_player_and_opponent(self, rel):\n",
    "        return self.mapToRange(math.degrees(math.atan2(rel[0], rel[1])))\n",
    "\n",
    "    def angleDiff(self, a1, a2):\n",
    "        d = a1 - a2\n",
    "        if d>180: d -= 360\n",
    "        if d<-180: d += 360\n",
    "        return d\n",
    "\n",
    "    def save_model(self, filename=\"dqn_model.pth\"):\n",
    "        os.makedirs(\"models1\", exist_ok=True)\n",
    "        torch.save(self.q_network.state_dict(), os.path.join(\"models1\", filename))\n",
    "\n",
    "    def load_model(self, filename=\"dqn_model.pth\"):\n",
    "        self.q_network.load_state_dict(torch.load(os.path.join(\"models1\", filename)))\n",
    "        self.q_network.eval()\n",
    "\n",
    "# ----------------------------\n",
    "# Training Loop\n",
    "# ----------------------------\n",
    "NUMBER_OF_EPISODES = 2000\n",
    "NUMBER_OF_STEPS    = 2500\n",
    "\n",
    "player_agent   = DQNAgent(11, 12)\n",
    "opponent_agent = DQNAgent(11, 12, opponent=True)\n",
    "\n",
    "def reset():\n",
    "    player_agent.environment_simulator.reset()\n",
    "    es = player_agent.environment_simulator\n",
    "    osim = opponent_agent.environment_simulator\n",
    "    osim.player_pos         = es.opponent_pos[:]\n",
    "    osim.player_health      = es.opponent_health\n",
    "    osim.player_orientation = es.opponent_orientation\n",
    "    osim.player_gun         = es.opponent_gun\n",
    "    osim.opponent_pos       = es.player_pos[:]\n",
    "    osim.opponent_health    = es.player_health\n",
    "    osim.opponent_orientation = es.player_orientation\n",
    "    osim.opponent_gun       = es.player_gun\n",
    "\n",
    "pretrained_path = \"models/player_winner.pth\"\n",
    "if os.path.exists(pretrained_path):\n",
    "    print(\"[INFO] Loading pretrained weights...\")\n",
    "    player_agent.load_model(\"winner.pth\")\n",
    "    opponent_agent.load_model(\"winner.pth\")\n",
    "\n",
    "for ep in range(NUMBER_OF_EPISODES):\n",
    "    player_agent.reset_metrics()\n",
    "    opponent_agent.reset_metrics()\n",
    "    p_total, o_total = 0.0, 0.0\n",
    "    p_done, o_done = False, False\n",
    "\n",
    "    for step in range(NUMBER_OF_STEPS):\n",
    "        p_ns, p_r, p_done = player_agent.step()\n",
    "        o_ns, o_r, o_done = opponent_agent.step()\n",
    "\n",
    "        p_total += p_r\n",
    "        o_total += o_r\n",
    "\n",
    "        if p_done or o_done:\n",
    "            break\n",
    "\n",
    "        # 同步環境\n",
    "        es = player_agent.environment_simulator\n",
    "        osim = opponent_agent.environment_simulator\n",
    "        osim.player_pos         = es.opponent_pos[:]\n",
    "        osim.player_health      = es.opponent_health\n",
    "        osim.player_orientation = es.opponent_orientation\n",
    "        osim.player_gun         = es.opponent_gun\n",
    "        osim.opponent_pos       = es.player_pos[:]\n",
    "        osim.opponent_health    = es.player_health\n",
    "        osim.opponent_orientation = es.player_orientation\n",
    "        osim.opponent_gun       = es.player_gun\n",
    "\n",
    "    # Episode 統計\n",
    "    print(f\"\\n=== Episode {ep} Summary ===\")\n",
    "    print(f\"Player  Reward: {p_total:.2f}\")\n",
    "    print(f\"  Hits: {player_agent.hit_count}, Misses: {player_agent.miss_count}, Kills: {player_agent.kill_count}\")\n",
    "    print(f\"  Moves: {player_agent.move_count}, TurnL: {player_agent.turn_left_count}, TurnR: {player_agent.turn_right_count}, Switch: {player_agent.switch_count}\")\n",
    "    print(f\"Opponent Reward: {o_total:.2f}\")\n",
    "    print(f\"  Hits: {opponent_agent.hit_count}, Misses: {opponent_agent.miss_count}, Kills: {opponent_agent.kill_count}\")\n",
    "    print(f\"  Moves: {opponent_agent.move_count}, TurnL: {opponent_agent.turn_left_count}, TurnR: {opponent_agent.turn_right_count}, Switch: {opponent_agent.switch_count}\")\n",
    "    print(\"===================================\\n\")\n",
    "\n",
    "    # 決定勝者並存檔權重\n",
    "    if p_done:\n",
    "        winner = opponent_agent\n",
    "        wname = 'opponent'\n",
    "    elif o_done:\n",
    "        winner = player_agent\n",
    "        wname = 'player'\n",
    "    else:\n",
    "        if p_total >= o_total:\n",
    "            winner = player_agent\n",
    "            wname = 'player'\n",
    "        else:\n",
    "            winner = opponent_agent\n",
    "            wname = 'opponent'\n",
    "    winner.save_model(f\"{wname}_winner_ep{ep}.pth\")\n",
    "    # 載入勝者權重給雙方\n",
    "    state_dict = winner.q_network.state_dict()\n",
    "    player_agent.q_network.load_state_dict(state_dict)\n",
    "    opponent_agent.q_network.load_state_dict(state_dict)\n",
    "    player_agent.update_target()\n",
    "    opponent_agent.update_target()\n",
    "\n",
    "    reset()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
